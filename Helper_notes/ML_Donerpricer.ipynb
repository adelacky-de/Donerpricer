{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EavRSn1Bk-m"
      },
      "source": [
        "-----\n",
        "\n",
        "# üåØ Machine Learning Lab: Doner Kebab Price Predictor\n",
        "\n",
        "Welcome to the Doner Detective Lab! In this session, you will transform into a Data Scientist to solve a tasty mystery: What actually drives the price of a Doner Kebab?\n",
        "\n",
        "Using Ridge Regression, you will teach a computer to find the mathematical \"sweet spot\" between historical patterns and real-world price spikes. You will learn to move from raw data to a stable predictive formula, discovering exactly how much \"brand,\" \"location,\" and \"timing\" weigh in on your final bill.\n",
        "\n",
        "-----\n",
        "\n",
        "## üèóÔ∏è Phase 1: The Prediction Equation (The Brain's Structure)\n",
        "\n",
        "Machines don't guess based on feelings; they use **Linear Models** as their logical foundation.\n",
        "\n",
        "### 1\\. The Mathematical Expression\n",
        "\n",
        "$$\\hat{y} = w_1x_1 + w_2x_2 + b$$\n",
        "\n",
        "### 2\\. Component Breakdown & Weight Selection ($w$)\n",
        "\n",
        "  * **$\\hat{y}$ (Y-hat)**: The predicted price (in Euro).\n",
        "  * **$x_1, x_2$**: Input **Features** (e.g., $x_1$ = is it Weekend?, $x_2$ = is it REWE?).\n",
        "  * **$w_1, w_2$ (Weights)**:\n",
        "      * **Definition**: The \"Influence\" of a feature. If $w_1$ is high, it means the \"Weekend\" is a major factor in price increases.\n",
        "      * **How are they chosen?**: At the very start, the machine chooses **random numbers** (e.g., 0.1). It starts \"clueless\" and improves through the \"Learning\" phase.\n",
        "  * **$b$ (Bias)**: The \"Base Price.\" Even without any extra factors, a Kebab has a fundamental cost.\n",
        "\n",
        "### 3\\. Real-world Example\n",
        "\n",
        "Assume initial weights: $w_1 = 0.5$, $w_2 = 0.5$, $b = 4.0$.\n",
        "If you buy on a **Weekend ($x_1=1$)** at **REWE ($x_2=1$)**:\n",
        "$$\\hat{y} = 0.5(1) + 0.5(1) + 4.0 = 5.5 \\text{ EUR}$$\n",
        "\n",
        "-----\n",
        "\n",
        "## üìâ Phase 2: The Loss Function (Measuring \"Pain\")\n",
        "\n",
        "After making a guess, we must tell the machine how \"bad\" it was.\n",
        "\n",
        "### 1\\. The Mathematical Expression (Mean Squared Error)\n",
        "\n",
        "$$Loss = (y - \\hat{y})^2$$\n",
        "\n",
        "### 2\\. Machine Preference: Lower Loss\n",
        "\n",
        "  * **Why?**: $Loss$ represents \"Error.\" The goal of Machine Learning is **Loss Minimization**.\n",
        "  * **$Loss = 0$** means the prediction matches reality perfectly. The machine will work tirelessly to reach the lowest possible Loss.\n",
        "\n",
        "### 3\\. Real-world Example\n",
        "\n",
        "True Price $y = 7.0$. Current Prediction $\\hat{y} = 5.0$.\n",
        "$Loss = (7.0 - 5.0)^2 = 4.0$. The machine sees **4.0** and feels \"pain,\" prompting it to adjust its weights.\n",
        "\n",
        "-----\n",
        "\n",
        "## üõ°Ô∏è Phase 3: Regularization (Preventing \"Rote Memorization\")\n",
        "\n",
        "If there is an outlier (e.g., a typo saying a Kebab costs ‚Ç¨100), a machine might over-adjust its weights to fit that error. This is called **Overfitting**.\n",
        "\n",
        "### 1\\. The Mathematical Expression (Ridge)\n",
        "\n",
        "$$Loss_{Total} = Loss_{MSE} + \\lambda \\sum w^2$$\n",
        "\n",
        "### 2\\. Machine Preference: Lower $Loss_{Total}$\n",
        "\n",
        "  * This is a **Balance Game**. $\\lambda$ (Lambda) is the \"Restraint\" we put on the machine.\n",
        "  * If the machine tries to make $w$ huge to fit an outlier, $\\lambda \\sum w^2$ will skyrocket. To keep the **Total Loss** low, the machine chooses to stay **rational** and ignore the outlier.\n",
        "\n",
        "-----\n",
        "\n",
        "## üîÑ Phase 4: Parameter Updates & Iteration (Getting Smarter)\n",
        "\n",
        "### 1\\. The Mathematical Expression\n",
        "\n",
        "$$w_{new} = w_{old} - \\alpha \\cdot \\text{Gradient}$$\n",
        "\n",
        "### 2\\. The Logic of Iteration\n",
        "\n",
        "The machine repeats a loop: **\"See Data $\\to$ Calculate Error $\\to$ Update Weights.\"**\n",
        "\n",
        "  * **One Epoch**: The machine has looked at all 80 rows of data once.\n",
        "  * **Convergence**: After hundreds of repetitions, the weights stop changing. The machine has found its \"Best Guess.\"\n",
        "  \n",
        "-----\n",
        "\n",
        "## üíª Practical Implementation (Python Code Flow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_PaER26Bk-y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeCV\n",
        "import database\n",
        "\n",
        "# 1. Prepare Ingredients (Feature Engineering)\n",
        "# Convert text to 0s and 1s (One-Hot Encoding)\n",
        "data = database.get_prices_by_item_and_brand(\"Doner Kebab\")\n",
        "X = pd.get_dummies(data.drop(columns=['price_eur', 'purchase_date']))\n",
        "y = data['price_eur']\n",
        "\n",
        "# 2. Fairness Training (Feature Scaling)\n",
        "# Math: z = (x - Œº) / œÉ\n",
        "# We shrink all numbers (e.g., 2000m vs 7 days) to a similar scale so Lambda is fair\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Automatic Lambda Search (Finding the Sweet Spot)\n",
        "# CV=5 means 5-fold cross-validation; the machine takes 5 exams to find the best Œª\n",
        "model = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)\n",
        "\n",
        "# 4. Train Model (Iteration Begins)\n",
        "# This line triggers the gradient descent and weight updates!\n",
        "model.fit(X_scaled, y)\n",
        "\n",
        "# 5. Output Results (Reading the Machine's Brain)\n",
        "print(f\"Base Price (Bias): {model.intercept_}\")\n",
        "print(f\"Learned Influence (Weights): {model.coef_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW09t6FWBk-z"
      },
      "source": [
        "-----\n",
        "\n",
        "# üïµÔ∏è‚Äç‚ôÇÔ∏è Detective Challenge: The Doner Mystery\n",
        "\n",
        "**Goal**: Use your new skills to find the \"Secret Price Rules\" in a file named `test_data.csv`.\n",
        "\n",
        "### Step 1: Loading the Clues\n",
        "\n",
        "**Task**: Use `pandas` to read the CSV file.\n",
        "\n",
        "  * **Hint**: Use `pd.read_csv()`. Look at your data with `.head()`.\n",
        "\n",
        "<!-- end list -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ6Bj6UiBk-z"
      },
      "outputs": [],
      "source": [
        "# Your Code Here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16LMSs7FBk-5"
      },
      "source": [
        "### Step 2: Translation (Engineering)\n",
        "\n",
        "**Task**: Convert dates and text to numbers.\n",
        "\n",
        "  * **Hint**: Use `pd.to_datetime()` for the date and `pd.get_dummies()` for the supermarkets.\n",
        "\n",
        "<!-- end list -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfbKlxNqBk-5"
      },
      "outputs": [],
      "source": [
        "# Your Code Here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCaPtl5pBk-6"
      },
      "source": [
        "### Step 3: The Level Playing Field (Scaling)\n",
        "\n",
        "**Task**: Scale your features so the machine doesn't get biased.\n",
        "\n",
        "  * **Hint**: Separate your target `y` (Price) first, then use `StandardScaler()`.\n",
        "  * **Math Connection**: Why do we do this? To make sure $\\lambda$ treats every $w$ equally\\!\n",
        "\n",
        "<!-- end list -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZhI44nDBk-6"
      },
      "outputs": [],
      "source": [
        "# Your Code Here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH3EYmgIBk-6"
      },
      "source": [
        "### Step 4: Training the Detective (Modeling)\n",
        "\n",
        "**Task**: Train a `RidgeCV` model.\n",
        "\n",
        "  * **Hint**: Provide a list of `alphas` (Lambdas) for the machine to try.\n",
        "\n",
        "<!-- end list -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9o_mGVyBk_A"
      },
      "outputs": [],
      "source": [
        "# Your Code Here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I2rs_GDBk_A"
      },
      "source": [
        "### Step 5: Cracking the Case (Interpretation)\n",
        "\n",
        "**Task**: Look at the Weights.\n",
        "\n",
        "  * **Final Challenge**: Find the top 3 features with the highest weights.\n",
        "  * **Conclusion**: Write one sentence: \"I discovered that if **[Condition]** happens, the Kebab price goes up significantly\\!\"\n",
        "\n",
        "<!-- end list -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtL69WwfBk_A"
      },
      "outputs": [],
      "source": [
        "# Your Code Here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE17jhD2Bk_A"
      },
      "source": [
        "-----\n",
        "\n",
        "### üåü Summary for Students\n",
        "\n",
        "1.  **Weights**: Start random, improve via **Gradient Descent**.\n",
        "2.  **Loss**: The lower the better\\!\n",
        "3.  **Iteration**: A marathon of \"Guessing and Correcting.\"\n",
        "4.  **Regularization ($\\lambda$)**: The \"Brake\" that prevents the machine from going crazy over outliers.\n",
        "\n",
        "-----\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
