{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyEl4gKkvJot"
      },
      "source": [
        "# üåØ Machine Learning Lab: Challenger Edition\n",
        "\n",
        "### Core Goal: Evolve from \"Basic Guessing\" to \"Memory-Based Professional Prediction\"\n",
        "\n",
        "In the basic lab, we taught the computer to look at \"Store Location\" to guess prices. But in the real world, prices fluctuate with time, seasons, and trends. Today, we are performing a **\"Brain Upgrade\"** to give your AI model the ability to observe historical patterns.\n",
        "\n",
        "-----\n",
        "\n",
        "## üßê 1. Why Upgrade? (Analogy Time)\n",
        "\n",
        "Imagine two assistants trying to guess the price of a Kebab:\n",
        "\n",
        "  * **Basic Assistant**: He only looks at \"Which supermarket was this bought from?\" He assumes prices at that store never change.\n",
        "  * **Challenger Assistant**: He doesn't just look at the store; he flips through his **notebook**. He thinks: \"What was the average price last week?\" and \"Has the price been jumpy lately?\"\n",
        "\n",
        "The **Challenger Assistant** is much more accurate because he has \"memory\" and \"observation skills.\"\n",
        "\n",
        "-----\n",
        "\n",
        "## üõ†Ô∏è 2. Advanced Implementation: Step-by-Step Improvements\n",
        "\n",
        "### Step A: Automated Labeling and \"Wearing Uniforms\" (Prefixing)\n",
        "\n",
        "In the advanced version, we let the computer automatically detect all locations and brands, giving them a **Prefix**.\n",
        "\n",
        "> **Why add a Prefix?**\n",
        "> To avoid \"Identity Confusion.\" If a Brand is named \"Central\" and a Location is also named \"Central,\" the computer might crash without a prefix. By adding prefixes like `brand_Central` and `location_Central`, it's like putting **uniforms** on the data. The computer won't get confused, and we can easily tell if a \"1\" represents a brand or a location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyFvH4_OvJou"
      },
      "outputs": [],
      "source": [
        "# Advanced Demo: Automatically tag categories with 0s and 1s and put on \"uniforms\" (prefixes)\n",
        "categorical_cols = ['brand_name', 'supermarket', 'location']\n",
        "df_dummies = pd.get_dummies(df, columns=categorical_cols, prefix=['brand', 'supermarket', 'location'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXZyfZ-7vJou"
      },
      "source": [
        "-----\n",
        "\n",
        "### Step B: Building \"Memory\" Features (Feature Engineering)\n",
        "\n",
        "This is where we create \"New Features\" the computer couldn't see before. This forms the ingredients for **`X_advanced`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9x_JUNuvJov"
      },
      "outputs": [],
      "source": [
        "# 0. CRITICAL: Sort by date! Otherwise 'rolling' memory is scrambled.\n",
        "df = df.sort_values(by='date')\n",
        "\n",
        "# 1. Time Features: Help the computer understand the day of the year and month (capturing seasonality)\n",
        "df_dummies['day_of_year'] = df['date'].dt.dayofyear\n",
        "df_dummies['month'] = df['date'].dt.month\n",
        "\n",
        "# 2. Historical Memory (Rolling Stats): Observe the average price and stability over the last 7 days\n",
        "# rolling_avg: Weekly trend / price_volatility: Is the price stable or jumpy?\n",
        "df_dummies['rolling_avg'] = df['price'].rolling(window=7, min_periods=1).mean()\n",
        "df_dummies['price_volatility'] = df['price'].rolling(window=7, min_periods=1).std().fillna(0)\n",
        "\n",
        "# 3. Physical Attributes: Consider the weight of the Kebab\n",
        "df_dummies['weight_grams'] = df_dummies['weight_grams'].fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWWz9A5xvJov"
      },
      "source": [
        "-----\n",
        "\n",
        "### Step C: The Magic Filter (Automated Feature Selection)\n",
        "\n",
        "This is the smartest line in your code. We use the \"uniforms\" (prefixes) we created earlier as a \"magnet\" to pull out all the tags we need at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEMtYLHMvJov"
      },
      "outputs": [],
      "source": [
        "# Define our other numerical features\n",
        "features = ['day_of_year', 'day_of_week', 'month', 'rolling_avg', 'price_volatility', 'weight_grams']\n",
        "\n",
        "# Use the \"uniform\" prefixes to automatically grab all category tags\n",
        "categorical_features = [col for col in df_dummies.columns if col.startswith(('brand_', 'supermarket_', 'location_'))]\n",
        "\n",
        "# Combine! This creates the final X_advanced\n",
        "X_advanced = df_dummies[features + categorical_features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp770KhQvJov"
      },
      "source": [
        "#### üîç How does the computer run this filter? (Step-by-Step)\n",
        "\n",
        "Imagine the computer sees this list of columns: `['price', 'brand_REWE', 'month', 'location_Berlin', 'weight_grams']`\n",
        "\n",
        "| Round | Column Checked (`col`) | Does it start with `brand_/supermarket_/location_`? | Result |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| 1 | `price` | ‚ùå No | Discard |\n",
        "| 2 | `brand_REWE` | ‚úÖ Yes (starts with `brand_`) | **Add to List\\!** |\n",
        "| 3 | `month` | ‚ùå No | Discard |\n",
        "| 4 | `location_Berlin` | ‚úÖ Yes (starts with `location_`) | **Add to List\\!** |\n",
        "| 5 | `weight_grams` | ‚ùå No | Discard |\n",
        "\n",
        "-----\n",
        "\n",
        "### Step D: The \"Fair Scale\" (Scaling to `X_scaled`)\n",
        "\n",
        "Why is `X_advanced` not enough? Why must we convert it to `X_scaled`?\n",
        "\n",
        "  * **`X_advanced` (Raw Ingredients)**: Contains huge numbers (Weight 500g) and tiny numbers (Month 1).\n",
        "  * **Scaling (Standardization)**: If we feed this directly to the computer, it will think \"bigger numbers are more important.\" We shrink/stretch all numbers to a similar size (usually between -3 and 3) so the computer can judge them fairly.\n",
        "\n",
        "<!-- end list -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWeZmpe1vJov"
      },
      "outputs": [],
      "source": [
        "# Pass through the \"Fair Scale\" (The Bridge)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_advanced) # X_advanced becomes X_scaled here\n",
        "\n",
        "# Feed to the brain for training\n",
        "# Note: In a real app, if you have < 5 rows of data, 'cv=5' will error.\n",
        "# You would adjust this number dynamically (e.g. cv=min(len(X_scaled), 5), or use a try-except block).\n",
        "model = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)\n",
        "model.fit(X_scaled, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FutureLoopMkdn"
      },
      "source": [
        "-----\n",
        "\n",
        "### Step E: The Crystal Ball (Predicting the Future)\n",
        "\n",
        "A trained model is useless if it only remembers the past. We need to create a **\"hypothetical future\"** dataframe to ask: \"What if I buy a Kebab next Tuesday?\"\n",
        "\n",
        "1.  **Generate Dates**: Create rows for the next 7 days.\n",
        "2.  **Carry Forward Memory**: Assume the `rolling_avg` and `price_volatility` stay the same as the last known day (for simplicity).\n",
        "3.  **Scale & Predict**: Run these future rows through the same `scaler` and `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FutureLoopCode"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "\n",
        "# 1. Create Future Dates\n",
        "today = datetime.now()\n",
        "future_dates = [today + timedelta(days=i) for i in range(7)]\n",
        "\n",
        "# 2. Build Future Data (Simplification: Copy last known stats)\n",
        "last_row = df_dummies.iloc[-1]\n",
        "future_data = {\n",
        "    'day_of_year': [d.timetuple().tm_yday for d in future_dates],\n",
        "    'day_of_week': [d.weekday() for d in future_dates],\n",
        "    'month': [d.month for d in future_dates],\n",
        "    'rolling_avg': [last_row['rolling_avg']] * 7,\n",
        "    'price_volatility': [last_row['price_volatility']] * 7,\n",
        "    'weight_grams': [last_row['weight_grams']] * 7\n",
        "}\n",
        "# Add dummy brand/location cols (all 0s or carry over)\n",
        "for col in categorical_features:\n",
        "    future_data[col] = [last_row[col]] * 7\n",
        "\n",
        "future_df = pd.DataFrame(future_data)\n",
        "# Ensure columns match training data exactly\n",
        "future_df = future_df[X_advanced.columns]\n",
        "\n",
        "# 3. Predict\n",
        "future_scaled = scaler.transform(future_df)\n",
        "predictions = model.predict(future_scaled)\n",
        "\n",
        "# Find the best day\n",
        "best_idx = np.argmin(predictions)\n",
        "best_day = future_dates[best_idx].strftime(\"%A\")\n",
        "predicted_price = predictions[best_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8vjnRCdvJov"
      },
      "source": [
        "-----\n",
        "\n",
        "## üèÅ 3. Final Results & Confidence Score\n",
        "\n",
        "The advanced model doesn't just give you a price; it tells you how much **\"Confidence\"** it has based on recent **Price Volatility**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9pSeUWuvJov"
      },
      "outputs": [],
      "source": [
        "# Confidence Formula: If prices have been jumping (high volatility), confidence goes down\n",
        "avg_volatility = df_dummies['price_volatility'].mean()\n",
        "\n",
        "# Note: '50' is a sensitivity setting. Change to '20' for a calmer AI (case-by-case), or '80' for a nervous one.\n",
        "confidence = max(0, min(100, int(100 - (avg_volatility * 50))))\n",
        "\n",
        "print(f\"--- Challenger Report ---\")\n",
        "print(f\"Predicted Best Price: {predicted_price:.2f} ‚Ç¨\")\n",
        "print(f\"Machine Confidence: {confidence}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsdKk_lavJov"
      },
      "source": [
        "-----\n",
        "\n",
        "## üïµÔ∏è‚Äç‚ôÇÔ∏è Critical Thinking (Discussion)\n",
        "\n",
        "1.  **Why Prefix?** If we have two labels both named \"Central\"‚Äîone is a brand and one is a location‚Äîwhat happens if we don't use prefixes?\n",
        "2.  **Filter Logic**: In the \"Step-by-Step\" table, why do we discard `price`? (Hint: Can a student take an exam while looking at the answer key?)\n",
        "3.  **Fairness**: If we skip Scaling, do you think the computer will listen more to `weight_grams` (500) or `month` (1)?\n",
        "4.  **Memory**: How does `rolling_avg` help the computer realize a shop is \"quietly raising its prices\"?\n",
        "\n",
        "-----\n",
        "\n",
        "**Summary**:\n",
        "The essence of Machine Learning is **\"Feature Engineering.\"** Through **`X_advanced`**, we gave the computer a broader vision and memory. Through **`X_scaled`**, we ensured the learning process was fair. This is the key process of evolving a simple \"calculator\" into a \"Professional AI\\!\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}